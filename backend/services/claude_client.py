"""
Brain - AI å®¢æˆ¶ç«¯
æ”¯æ´ OpenRouter (æ¨è–¦) å’Œ Anthropic ç›´é€£å…©ç¨®æ¨¡å¼
å¯¦ä½œ LLM Routing æ¨¡å‹åˆ†æµåŠŸèƒ½
"""
import json
from typing import Dict, Optional
from openai import AsyncOpenAI
from anthropic import Anthropic
from config import settings


class ClaudeClient:
    """AI API å®¢æˆ¶ç«¯ - æ”¯æ´ OpenRouter å’Œ Anthropic"""

    def __init__(self):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯"""
        self.mock_mode = False
        self.provider = settings.AI_PROVIDER
        self.model = settings.CLAUDE_MODEL
        self.openrouter_client = None
        self.anthropic_client = None

        # æ ¹æ“š Provider è¨­å®šåˆå§‹åŒ–
        if self.provider == "openrouter":
            if not settings.OPENROUTER_API_KEY:
                print("è­¦å‘Šï¼šOPENROUTER_API_KEY æœªè¨­å®šï¼Œä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
                self.mock_mode = True
            else:
                self.openrouter_client = AsyncOpenAI(
                    base_url="https://openrouter.ai/api/v1",
                    api_key=settings.OPENROUTER_API_KEY,
                    default_headers={
                        "HTTP-Referer": "https://brain.yourspce.org",
                        "X-Title": "Hour Jungle Brain"
                    }
                )
                print(f"âœ… OpenRouter å®¢æˆ¶ç«¯å·²åˆå§‹åŒ–")
                print(f"   Smart Model: {settings.MODEL_SMART}")
                print(f"   Fast Model: {settings.MODEL_FAST}")
        else:
            # Anthropic ç›´é€£æ¨¡å¼
            if not settings.ANTHROPIC_API_KEY:
                print("è­¦å‘Šï¼šANTHROPIC_API_KEY æœªè¨­å®šï¼Œä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
                self.mock_mode = True
            else:
                self.anthropic_client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)
                print(f"âœ… Anthropic å®¢æˆ¶ç«¯å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: {self.model}")

    def _parse_json_response(self, content: str) -> Optional[Dict]:
        """
        ç©©å¥çš„ JSON è§£æï¼Œè™•ç†å„ç¨®æ ¼å¼çš„ LLM å›æ‡‰

        æ”¯æ´ï¼š
        - ç´” JSON
        - Markdown code block åŒ…è£¹çš„ JSON
        - æœ‰å‰å¾Œæ–‡å­—çš„ JSON
        """
        import re

        content = content.strip()

        # å˜—è©¦ 1ï¼šç›´æ¥è§£æ
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass

        # å˜—è©¦ 2ï¼šç§»é™¤ markdown code blocks
        if "```" in content:
            # æå– code block å…§å®¹
            match = re.search(r'```(?:json)?\s*([\s\S]*?)```', content)
            if match:
                try:
                    return json.loads(match.group(1).strip())
                except json.JSONDecodeError:
                    pass

        # å˜—è©¦ 3ï¼šå°‹æ‰¾ JSON ç‰©ä»¶ï¼ˆ{ ... }ï¼‰
        match = re.search(r'\{[\s\S]*"draft"[\s\S]*\}', content)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

        # å˜—è©¦ 4ï¼šæ‰¾ç¬¬ä¸€å€‹ { åˆ°æœ€å¾Œä¸€å€‹ }
        first_brace = content.find('{')
        last_brace = content.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            try:
                return json.loads(content[first_brace:last_brace + 1])
            except json.JSONDecodeError:
                pass

        # å…¨éƒ¨å¤±æ•—
        print(f"âš ï¸ JSON è§£æå¤±æ•—ï¼ŒåŸå§‹å…§å®¹å‰ 200 å­—: {content[:200]}")
        return None

    async def route_task(self, message: str) -> Dict:
        """
        [LLM Routing ç¬¬ä¸€æ­¥] è·¯ç”±åˆ†æï¼šåˆ¤æ–·ä»»å‹™è¤‡é›œåº¦
        ä½¿ç”¨ Smart Model é€²è¡Œç²¾æº–åˆ¤æ–·
        """
        from brain.prompts import build_router_prompt

        if self.mock_mode:
            return {"complexity": "COMPLEX", "reason": "æ¨¡æ“¬æ¨¡å¼", "suggested_intent": "å…¶ä»–"}

        if not settings.ENABLE_ROUTING:
            # æœªå•Ÿç”¨åˆ†æµï¼Œå…¨éƒ¨ä½¿ç”¨ Smart Model
            return {"complexity": "COMPLEX", "reason": "åˆ†æµæœªå•Ÿç”¨", "suggested_intent": "å…¶ä»–"}

        try:
            prompt = build_router_prompt(message)

            if self.provider == "openrouter":
                response = await self.openrouter_client.chat.completions.create(
                    model=settings.MODEL_SMART,
                    messages=[
                        {"role": "system", "content": "You are a task router. Output JSON only."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.0,
                    max_tokens=200
                )
                content = response.choices[0].message.content
                # æå–ç”¨é‡è³‡è¨Š
                usage = {
                    "input_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "output_tokens": response.usage.completion_tokens if response.usage else 0,
                    "model": settings.MODEL_SMART
                }
            else:
                # Anthropic ç›´é€£
                response = self.anthropic_client.messages.create(
                    model=self.model,
                    max_tokens=200,
                    temperature=0.0,
                    messages=[{"role": "user", "content": prompt}]
                )
                content = response.content[0].text
                usage = {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                    "model": self.model
                }

            # å˜—è©¦è§£æ JSONï¼ˆä½¿ç”¨ç©©å¥çš„è§£ææ–¹æ³•ï¼‰
            result = self._parse_json_response(content)
            if result:
                result["_usage"] = usage
                return result
            else:
                return {
                    "complexity": "COMPLEX",
                    "reason": "JSONè§£æå¤±æ•—",
                    "suggested_intent": "å…¶ä»–",
                    "_usage": usage
                }

        except Exception as e:
            print(f"âŒ è·¯ç”±åˆ¤æ–·å¤±æ•—ï¼Œé è¨­ç‚ºè¤‡é›œæ¨¡å¼: {e}")
            return {"complexity": "COMPLEX", "reason": f"åˆ†æå¤±æ•—: {str(e)[:20]}", "suggested_intent": "å…¶ä»–"}

    async def generate_draft(
        self,
        message: str,
        sender_name: str,
        source: str,
        context: Optional[Dict] = None,
        model: str = None,
        conversation_history: str = "",
        rag_context: str = "",
        customer_context: str = ""
    ) -> Dict:
        """
        [LLM Routing ç¬¬äºŒæ­¥] ç”Ÿæˆå›è¦†è‰ç¨¿
        å¯æŒ‡å®šä½¿ç”¨ç‰¹å®šæ¨¡å‹

        Args:
            conversation_history: æ ¼å¼åŒ–çš„å°è©±æ­·å²å­—ä¸²
            rag_context: RAG æª¢ç´¢çš„ç›¸é—œçŸ¥è­˜
            customer_context: CRM å®¢æˆ¶è³‡æ–™ï¼ˆä¾†è‡ª Jungleï¼‰
        """
        from brain.prompts import build_draft_prompt

        # æ±ºå®šä½¿ç”¨å“ªå€‹æ¨¡å‹
        if self.provider == "openrouter":
            target_model = model or settings.MODEL_SMART
        else:
            target_model = self.model

        # æ¨¡æ“¬æ¨¡å¼
        if self.mock_mode:
            return {
                "intent": "è©¢åƒ¹",
                "strategy": "SPIN-S äº†è§£å®¢æˆ¶ç¾æ³ï¼ˆæ¨¡æ“¬æ¨¡å¼ï¼‰",
                "draft": f"æ‚¨å¥½ï½å› ç‚ºç™»è¨˜éœ€è¦ç¶“éç¶“æ¿Ÿéƒ¨å’Œåœ‹ç¨…å±€ï¼Œéœ€è¦å…ˆäº†è§£æ‚¨ç›®å‰çš„æƒ…æ³ï¼š\n\nè«‹å•æ‚¨æ˜¯æ–°è¨­ç«‹é‚„æ˜¯é·å€å‘¢ï¼Ÿï¼ˆå·²æœ‰çµ±ç·¨è«‹ç›´æ¥æä¾›ï¼‰\n\næ–¹ä¾¿ LINE é€šè©±è·Ÿæ‚¨ç¢ºèªå—ï¼Ÿ",
                "next_action": "ç­‰å¾…å®¢æˆ¶å›è¦†åŸºæœ¬è³‡è¨Š",
                "_usage": {"input_tokens": 0, "output_tokens": 0, "model": "mock"}
            }

        # å»ºç«‹æç¤ºè©ï¼ˆåŒ…å«å°è©±æ­·å² + RAG çŸ¥è­˜ + å®¢æˆ¶è³‡æ–™ï¼‰
        prompt = build_draft_prompt(
            content=message,
            sender_name=sender_name,
            source=source,
            conversation_history=conversation_history,
            rag_context=rag_context,
            customer_context=customer_context
        )

        try:
            if self.provider == "openrouter":
                # å»ºç«‹ API åƒæ•¸
                api_params = {
                    "model": target_model,
                    "messages": [
                        {"role": "system", "content": "You are a helpful customer service assistant for Hour Jungle shared office. Output JSON only."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 16000 if settings.ENABLE_EXTENDED_THINKING else 2000
                }

                # OpenRouter æ”¯æ´ reasoning åƒæ•¸ï¼ˆé©ç”¨æ–¼ Claude 3.7+, Sonnet 4.5 ç­‰ï¼‰
                if settings.ENABLE_EXTENDED_THINKING:
                    api_params["extra_body"] = {
                        "reasoning": {
                            "max_tokens": settings.THINKING_BUDGET_TOKENS
                        }
                    }
                    print(f"ğŸ§  å•Ÿç”¨ Extended Thinking (budget: {settings.THINKING_BUDGET_TOKENS} tokens)")

                response = await self.openrouter_client.chat.completions.create(**api_params)
                content = response.choices[0].message.content
                usage = {
                    "input_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "output_tokens": response.usage.completion_tokens if response.usage else 0,
                    "model": target_model
                }
            else:
                # Anthropic ç›´é€£
                api_params = {
                    "model": target_model,
                    "max_tokens": 16000 if settings.ENABLE_EXTENDED_THINKING else 2000,
                    "temperature": 0.7,
                    "messages": [{"role": "user", "content": prompt}]
                }

                if settings.ENABLE_EXTENDED_THINKING:
                    api_params["thinking"] = {
                        "type": "enabled",
                        "budget_tokens": settings.THINKING_BUDGET_TOKENS
                    }

                response = self.anthropic_client.messages.create(**api_params)
                content = response.content[0].text
                usage = {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                    "model": target_model
                }

            # å˜—è©¦è§£æ JSONï¼ˆæ›´ç©©å¥çš„è§£æï¼‰
            result = self._parse_json_response(content)
            if result is None:
                # è§£æå¤±æ•—ï¼Œå˜—è©¦æå–ç´”æ–‡å­—ä½œç‚ºè‰ç¨¿
                clean_content = content.strip()
                # ç§»é™¤ markdown code blocks
                if "```" in clean_content:
                    import re
                    clean_content = re.sub(r'```(?:json)?\s*', '', clean_content)
                    clean_content = clean_content.replace('```', '').strip()
                result = {
                    "intent": "å…¶ä»–",
                    "strategy": "ç³»çµ±è‡ªå‹•ç”Ÿæˆï¼ˆJSONè§£æå¤±æ•—ï¼‰",
                    "draft": clean_content,
                    "next_action": "äººå·¥å¯©æ ¸"
                }

            result["_usage"] = usage
            return result

        except Exception as e:
            raise Exception(f"AI API èª¿ç”¨å¤±æ•— ({target_model}): {str(e)}")

    async def analyze_modification(
        self,
        original: str,
        final: str
    ) -> str:
        """
        åˆ†æäººå·¥ä¿®æ”¹åŸå› 
        """
        from brain.prompts import MODIFICATION_ANALYSIS_PROMPT

        if self.mock_mode:
            return "èª¿æ•´èªæ°£ï¼Œä½¿å›è¦†æ›´è¦ªåˆ‡è‡ªç„¶ï¼ˆæ¨¡æ“¬æ¨¡å¼ï¼‰"

        prompt = MODIFICATION_ANALYSIS_PROMPT.format(
            original=original,
            final=final
        )

        try:
            if self.provider == "openrouter":
                # åˆ†æç”¨ Fast Model å°±å¤ äº†
                response = await self.openrouter_client.chat.completions.create(
                    model=settings.MODEL_FAST,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5,
                    max_tokens=200
                )
                return response.choices[0].message.content.strip()
            else:
                response = self.anthropic_client.messages.create(
                    model=self.model,
                    max_tokens=200,
                    temperature=0.5,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text.strip()

        except Exception as e:
            return f"åˆ†æå¤±æ•—: {str(e)}"


# å…¨åŸŸ Claude å®¢æˆ¶ç«¯å¯¦ä¾‹
_claude_client: Optional[ClaudeClient] = None


def get_claude_client() -> ClaudeClient:
    """å–å¾— Claude å®¢æˆ¶ç«¯å–®ä¾‹"""
    global _claude_client
    if _claude_client is None:
        _claude_client = ClaudeClient()
    return _claude_client
