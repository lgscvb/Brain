"""
Brain - çŸ¥è­˜åº«åˆå§‹åŒ–è…³æœ¬
å¾ JSON æ–‡ä»¶è§£æä¸¦åŒ¯å…¥çŸ¥è­˜åˆ°è³‡æ–™åº«
"""
import asyncio
import json
import sys
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from db.models import Base, KnowledgeChunk
from services.embedding_client import get_embedding_client
from config import settings


class KnowledgeImporter:
    """çŸ¥è­˜åº«åŒ¯å…¥å™¨"""

    def __init__(self, session: AsyncSession):
        self.session = session
        self.embedding_client = get_embedding_client()
        self.imported_count = 0
        self.skipped_count = 0

    async def import_from_sales_mindmap(self, file_path: str) -> int:
        """
        å¾ sales_mindmap.json åŒ¯å…¥çŸ¥è­˜

        è§£æçµæ§‹ï¼š
        - spin_framework.phases.{S,P,I,N}.question_bank -> spin_question
        - service_types[].components[].items -> value_prop, objection, etc.
        - general_techniques[].tactics -> tactics
        """
        print(f"\nğŸ“š æ­£åœ¨åŒ¯å…¥ sales_mindmap.json...")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        mindmap = data.get("sales_mindmap", {})

        # 1. åŒ¯å…¥ SPIN å•é¡Œåº«
        await self._import_spin_questions(mindmap.get("spin_framework", {}).get("phases", {}))

        # 2. åŒ¯å…¥æœå‹™é¡å‹ç›¸é—œå…§å®¹
        await self._import_service_content(mindmap.get("service_types", []))

        # 3. åŒ¯å…¥é€šç”¨æŠ€å·§
        await self._import_techniques(mindmap.get("general_techniques", []))

        # 4. åŒ¯å…¥å¸¸è¦‹æƒ…å¢ƒ
        await self._import_scenarios(mindmap.get("spin_framework", {}).get("common_scenarios", []))

        return self.imported_count

    async def _import_spin_questions(self, phases: Dict):
        """åŒ¯å…¥ SPIN å„éšæ®µå•é¡Œ"""
        phase_names = {
            "S": "Situation",
            "P": "Problem",
            "I": "Implication",
            "N": "Need-payoff"
        }

        for phase_key, phase_data in phases.items():
            phase_name = phase_names.get(phase_key, phase_key)
            question_bank = phase_data.get("question_bank", {})

            for question_type, questions in question_bank.items():
                for question in questions:
                    await self._add_chunk(
                        content=question,
                        category="spin_question",
                        sub_category=phase_key,
                        metadata={
                            "phase_name": phase_name,
                            "question_type": question_type,
                            "purpose": phase_data.get("purpose", ""),
                            "tone": phase_data.get("tone", "")
                        }
                    )

    async def _import_service_content(self, service_types: List):
        """åŒ¯å…¥æœå‹™ç›¸é—œå…§å®¹"""
        for service in service_types:
            service_id = service.get("id", "")
            service_name = service.get("name", "")

            for component in service.get("components", []):
                component_type = component.get("type", "")

                # æ˜ å°„åˆ°æˆ‘å€‘çš„åˆ†é¡
                category_map = {
                    "value_statement": "value_prop",
                    "differentiation": "value_prop",
                    "concern_handling": "objection",
                    "call_to_action": "value_prop"
                }
                category = category_map.get(component_type, "service_info")

                for item in component.get("items", []):
                    await self._add_chunk(
                        content=item.get("content", ""),
                        category=category,
                        sub_category=component_type,
                        service_type=service_id,
                        metadata={
                            "service_name": service_name,
                            "item_id": item.get("id", ""),
                            "usage_context": item.get("usage_context", []),
                            "spin_phase": item.get("spin_phase", "")
                        }
                    )

    async def _import_techniques(self, techniques: List):
        """åŒ¯å…¥éŠ·å”®æŠ€å·§"""
        for technique in techniques:
            technique_name = technique.get("name", "")

            for tactic in technique.get("tactics", []):
                await self._add_chunk(
                    content=tactic.get("content", ""),
                    category="tactics",
                    sub_category=technique.get("id", ""),
                    metadata={
                        "technique_name": technique_name,
                        "spin_alignment": technique.get("spin_alignment", ""),
                        "usage_context": tactic.get("usage_context", []),
                        "spin_note": tactic.get("spin_note", "")
                    }
                )

    async def _import_scenarios(self, scenarios: List):
        """åŒ¯å…¥å¸¸è¦‹æƒ…å¢ƒ"""
        for scenario in scenarios:
            # æƒ…å¢ƒæè¿°
            await self._add_chunk(
                content=f"æƒ…å¢ƒï¼š{scenario.get('scenario', '')}\nå›è¦†ç¯„ä¾‹ï¼š{scenario.get('example_response', '')}",
                category="scenario",
                sub_category=scenario.get("spin_approach", ""),
                metadata={
                    "next_questions": scenario.get("next_questions", [])
                }
            )

    async def import_from_logic_tree(self, file_path: str) -> int:
        """
        å¾ logic_tree.json åŒ¯å…¥çŸ¥è­˜

        è§£æçµæ§‹ï¼š
        - root_nodes[].children[...].spin_questions -> spin_question
        """
        print(f"\nğŸ“š æ­£åœ¨åŒ¯å…¥ logic_tree.json...")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        logic_tree = data.get("logic_tree", {})

        # éè¿´åŒ¯å…¥æ‰€æœ‰ç¯€é»çš„ SPIN å•é¡Œ
        await self._import_tree_nodes(logic_tree.get("root_nodes", []))

        return self.imported_count

    async def _import_tree_nodes(self, nodes: List, parent_name: str = ""):
        """éè¿´åŒ¯å…¥æ¨¹ç‹€çµæ§‹çš„ç¯€é»"""
        for node in nodes:
            node_name = node.get("name", "")
            node_id = node.get("id", "")
            service_type = None

            # åˆ¤æ–·æœå‹™é¡å‹
            if "address" in node_id or "ç™»è¨˜" in node_name:
                service_type = "address_service"
            elif "coworking" in node_id or "å…±äº«" in node_name:
                service_type = "coworking"
            elif "office" in node_id or "è¾¦å…¬å®¤" in node_name:
                service_type = "private_office"

            # åŒ¯å…¥æ­¤ç¯€é»çš„ SPIN å•é¡Œ
            spin_questions = node.get("spin_questions", {})
            for phase, questions in spin_questions.items():
                for question in questions:
                    await self._add_chunk(
                        content=question,
                        category="spin_question",
                        sub_category=phase,
                        service_type=service_type,
                        metadata={
                            "node_id": node_id,
                            "node_name": node_name,
                            "parent": parent_name,
                            "keywords": node.get("keywords", [])
                        }
                    )

            # éè¿´è™•ç†å­ç¯€é»
            children = node.get("children", [])
            if children:
                await self._import_tree_nodes(children, node_name)

    async def import_from_training_data(self, file_path: str, limit: int = 50) -> int:
        """
        å¾ training_data.json åŒ¯å…¥å°è©±ç¯„ä¾‹

        Args:
            file_path: æ–‡ä»¶è·¯å¾‘
            limit: æœ€å¤§åŒ¯å…¥æ•¸é‡
        """
        print(f"\nğŸ“š æ­£åœ¨åŒ¯å…¥ training_data.json (é™åˆ¶ {limit} å‰‡)...")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        conversations = data.get("conversations", [])

        count = 0
        for conv in conversations[:limit]:
            topic = conv.get("topic", "")
            tags = conv.get("tags", [])

            for turn in conv.get("turns", []):
                if turn.get("role") == "assistant":
                    content = turn.get("content", "")
                    await self._add_chunk(
                        content=f"[{topic}] {content}",
                        category="example_response",
                        sub_category=turn.get("dialog_act", ""),
                        metadata={
                            "conversation_id": conv.get("conversation_id", ""),
                            "topic": topic,
                            "tags": tags,
                            "strategy": turn.get("strategy", "")
                        }
                    )
                    count += 1

        print(f"   åŒ¯å…¥äº† {count} å‰‡å°è©±ç¯„ä¾‹")
        return count

    async def _add_chunk(
        self,
        content: str,
        category: str,
        sub_category: str = None,
        service_type: str = None,
        metadata: Dict = None
    ):
        """æ·»åŠ å–®å€‹çŸ¥è­˜ chunk"""
        if not content or len(content.strip()) < 5:
            self.skipped_count += 1
            return

        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡è¤‡ï¼‰
        from sqlalchemy import select
        existing = await self.session.execute(
            select(KnowledgeChunk).where(
                KnowledgeChunk.content == content,
                KnowledgeChunk.category == category
            )
        )
        if existing.scalar_one_or_none():
            self.skipped_count += 1
            return

        # ç”Ÿæˆ Embedding
        embedding = await self.embedding_client.embed_text(content)

        chunk = KnowledgeChunk(
            content=content,
            category=category,
            sub_category=sub_category,
            service_type=service_type,
            extra_data=metadata or {},
            embedding_json=embedding,
            is_active=True
        )

        self.session.add(chunk)
        self.imported_count += 1

        # æ¯ 50 å€‹æäº¤ä¸€æ¬¡
        if self.imported_count % 50 == 0:
            await self.session.commit()
            print(f"   å·²åŒ¯å…¥ {self.imported_count} æ¢...")


async def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("Brain çŸ¥è­˜åº«åˆå§‹åŒ–å·¥å…·")
    print("=" * 60)

    # è³‡æ–™åº«é€£æ¥
    database_url = settings.DATABASE_URL
    # è½‰æ›ç‚º async driverï¼ˆå¦‚æœé‚„æ²’æœ‰ï¼‰
    if database_url.startswith("postgresql://"):
        database_url = database_url.replace("postgresql://", "postgresql+asyncpg://", 1)
    elif database_url.startswith("sqlite://") and "+aiosqlite" not in database_url:
        database_url = database_url.replace("sqlite://", "sqlite+aiosqlite://", 1)
    # å¦‚æœå·²ç¶“æ˜¯ sqlite+aiosqlite:// å‰‡ä¸éœ€è¦è½‰æ›

    engine = create_async_engine(database_url)
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

    # å»ºç«‹è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # æ‰¾åˆ° JSON æ–‡ä»¶ï¼ˆæ”¯æ´å¤šç¨®è·¯å¾‘ï¼‰
    def find_json_file(filename: str) -> Path:
        """æœå°‹ JSON æª”æ¡ˆï¼Œæ”¯æ´æœ¬åœ°é–‹ç™¼å’Œ Docker ç’°å¢ƒ"""
        search_paths = [
            Path(__file__).parent.parent.parent / filename,  # æœ¬åœ°: brain/
            Path(__file__).parent.parent / filename,          # Docker: /app/
            Path.cwd() / filename,                            # ç•¶å‰ç›®éŒ„
            Path.cwd().parent / filename,                     # çˆ¶ç›®éŒ„
        ]
        for path in search_paths:
            if path.exists():
                return path
        return search_paths[0]  # å›å‚³ç¬¬ä¸€å€‹è·¯å¾‘ç”¨æ–¼éŒ¯èª¤è¨Šæ¯

    sales_mindmap_path = find_json_file("sales_mindmap.json")
    logic_tree_path = find_json_file("logic_tree.json")
    training_data_path = find_json_file("training_data.json")

    async with async_session() as session:
        importer = KnowledgeImporter(session)

        # åŒ¯å…¥ sales_mindmap.json
        if sales_mindmap_path.exists():
            await importer.import_from_sales_mindmap(str(sales_mindmap_path))
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° {sales_mindmap_path}")

        # åŒ¯å…¥ logic_tree.json
        if logic_tree_path.exists():
            await importer.import_from_logic_tree(str(logic_tree_path))
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° {logic_tree_path}")

        # åŒ¯å…¥ training_data.jsonï¼ˆé™åˆ¶æ•¸é‡ï¼‰
        if training_data_path.exists():
            await importer.import_from_training_data(str(training_data_path), limit=30)
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° {training_data_path}")

        # æœ€çµ‚æäº¤
        await session.commit()

        print("\n" + "=" * 60)
        print(f"âœ… åŒ¯å…¥å®Œæˆ!")
        print(f"   - æˆåŠŸåŒ¯å…¥: {importer.imported_count} æ¢")
        print(f"   - ç•¥éï¼ˆé‡è¤‡/ç©ºç™½ï¼‰: {importer.skipped_count} æ¢")
        print("=" * 60)

    await engine.dispose()


if __name__ == "__main__":
    asyncio.run(main())
