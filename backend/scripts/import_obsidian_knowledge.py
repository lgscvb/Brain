"""
Brain - Obsidian çŸ¥è­˜åº«åŒ¯å…¥è…³æœ¬
å¾ Obsidian Vault çš„ Markdown æª”æ¡ˆåŒ¯å…¥çŸ¥è­˜åˆ° Brain RAG ç³»çµ±

ç”¨æ³•ï¼š
    # Dry Runï¼ˆé è¦½ï¼Œä¸å¯¦éš›å¯«å…¥ï¼‰
    python scripts/import_obsidian_knowledge.py /path/to/obsidian/vault --dry-run

    # å¯¦éš›åŒ¯å…¥
    python scripts/import_obsidian_knowledge.py /path/to/obsidian/vault

åŠŸèƒ½ï¼š
    1. éè¿´è®€å– .md æª”æ¡ˆ
    2. æŒ‰ ## æ¨™é¡Œåˆ‡å¡Šï¼ˆæ¯å¡Šç¨ç«‹æª¢ç´¢ï¼‰
    3. æå– YAML frontmatter ä½œç‚º metadata
    4. æ¸…ç† Obsidian ç‰¹æœ‰èªæ³•ï¼ˆwikilinksã€calloutsï¼‰
    5. å»é‡æª¢æŸ¥ï¼ˆcontent + category å®Œå…¨ç›¸åŒå‰‡è·³éï¼‰
    6. ç”Ÿæˆ Embedding ä¸¦å­˜å…¥ knowledge_chunks è¡¨
"""
import argparse
import asyncio
import re
import sys
from pathlib import Path
from typing import List, Dict, Optional, Any

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

# åˆ‡æ›å·¥ä½œç›®éŒ„åˆ° backendï¼Œç¢ºä¿ .env èƒ½è¢«æ­£ç¢ºè®€å–
import os
os.chdir(backend_dir)

# æ‰‹å‹•è¼‰å…¥ .envï¼ˆåœ¨ config æ¨¡çµ„ä¹‹å‰ï¼‰
from dotenv import load_dotenv
env_path = backend_dir / ".env"
if env_path.exists():
    load_dotenv(env_path, override=True)
    print(f"âœ… è¼‰å…¥ç’°å¢ƒè®Šæ•¸: {env_path}")

import yaml
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select
from db.models import Base, KnowledgeChunk
from services.embedding_client import get_embedding_client
from config import settings


# =============================================================================
# é…ç½®
# =============================================================================

# åˆ†é¡æ˜ å°„ï¼šæ ¹æ“šè·¯å¾‘è‡ªå‹•åˆ¤æ–·åˆ†é¡
CATEGORY_MAP = {
    "å…¬å¸è¨­ç«‹": "service_info",
    "ç¨…å‹™èˆ‡é¢¨æ§": "faq",
    "ç‰¹è¨±è¡Œæ¥­": "service_info",
    "å€Ÿå€ç™»è¨˜": "service_info",
    "å‹å¥ä¿": "faq",
    "éŠ·å”®æŠ€å·§": "tactics",
    "ç‡Ÿé‹è³‡è¨Š": "service_info",
}

# è·³éçš„æª”æ¡ˆï¼ˆå·²æœ‰æ›´å®Œæ•´ç‰ˆæœ¬åœ¨ sales_mindmap.json æˆ–ä¸é©åˆä½œç‚ºçŸ¥è­˜åº«ï¼‰
SKIP_FILES = [
    "æ¥­å‹™SOP.md",
    "éŠ·å”®è©±è¡“æŒ‡å—.md",
    "SPINéŠ·å”®æ³•å¿ƒæ™ºåœ–.md",
    "å¾…è¾¦äº‹é ….md",
    "README.md",
]

# è·³éçš„ç›®éŒ„ï¼ˆå…§éƒ¨ç‡Ÿé‹è³‡æ–™ï¼Œä¸é©åˆä½œç‚ºå®¢æœçŸ¥è­˜ï¼‰
SKIP_DIRECTORIES = [
    "å®¢æˆ¶é·å‡º",      # å€‹åˆ¥å®¢æˆ¶æ¡ˆä»¶
    "ç³»çµ±é–‹ç™¼",      # å…§éƒ¨é–‹ç™¼ç­†è¨˜
    "å°ˆæ¡ˆ",          # å°ˆæ¡ˆç®¡ç†
    "æœƒè­°ç´€éŒ„",      # å…§éƒ¨æœƒè­°
]

# æœ€å°å…§å®¹é•·åº¦ï¼ˆå¤ªçŸ­çš„å€å¡Šæ²’æœ‰æ„ç¾©ï¼‰
MIN_CONTENT_LENGTH = 50


# =============================================================================
# Markdown è§£æ
# =============================================================================

def sanitize_for_json(obj: Any) -> Any:
    """
    å°‡ç‰©ä»¶è½‰æ›ç‚º JSON å¯åºåˆ—åŒ–çš„æ ¼å¼

    è™•ç†ï¼š
    - datetime.date â†’ ISO å­—ä¸²
    - datetime.datetime â†’ ISO å­—ä¸²
    - å…¶ä»–ä¿æŒä¸è®Š
    """
    from datetime import date, datetime

    if isinstance(obj, (date, datetime)):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: sanitize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [sanitize_for_json(item) for item in obj]
    return obj


def parse_frontmatter(content: str) -> tuple[Dict[str, Any], str]:
    """
    æå– YAML frontmatter

    Returns:
        (frontmatter_dict, remaining_content)
    """
    frontmatter = {}

    if content.startswith('---'):
        parts = content.split('---', 2)
        if len(parts) >= 3:
            try:
                raw_frontmatter = yaml.safe_load(parts[1]) or {}
                # å°‡æ—¥æœŸç­‰ä¸å¯åºåˆ—åŒ–çš„é¡å‹è½‰æ›ç‚ºå­—ä¸²
                frontmatter = sanitize_for_json(raw_frontmatter)
            except yaml.YAMLError:
                pass  # è§£æå¤±æ•—å°±ç•¥é
            content = parts[2]

    return frontmatter, content


def clean_obsidian_syntax(text: str) -> str:
    """
    æ¸…ç† Obsidian ç‰¹æœ‰èªæ³•ï¼Œä¿ç•™æ ¸å¿ƒå…§å®¹

    è™•ç†ï¼š
    - [[wikilinks]] â†’ ç´”æ–‡å­—
    - [[link|alias]] â†’ alias
    - > [!tip] callout â†’ ä¿ç•™å…§å®¹ï¼Œç§»é™¤èªæ³•
    - ![[embed]] â†’ ç§»é™¤
    - #tags â†’ ç§»é™¤
    """
    # ç§»é™¤åµŒå…¥ ![[...]]
    text = re.sub(r'!\[\[([^\]]+)\]\]', '', text)

    # [[link|alias]] â†’ alias
    text = re.sub(r'\[\[([^\]|]+)\|([^\]]+)\]\]', r'\2', text)

    # [[wikilinks]] â†’ ç´”æ–‡å­—
    text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)

    # ç§»é™¤ callout æ¨™è¨˜è¡Œ > [!xxx] xxxï¼Œä½†ä¿ç•™å…¶å¾Œçš„å¼•ç”¨å…§å®¹
    text = re.sub(r'> \[!(\w+)\][^\n]*\n', '', text)

    # ç§»é™¤è¡Œå…§ #tagsï¼ˆä½†ä¿ç•™æ¨™é¡Œçš„ #ï¼‰
    text = re.sub(r'(?<!\n)#[\w\-]+(?:\s|$)', ' ', text)

    # ç§»é™¤å¤šé¤˜ç©ºè¡Œ
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


def parse_markdown_to_chunks(file_path: Path) -> List[Dict[str, Any]]:
    """
    è§£æ Markdown æª”æ¡ˆï¼ŒæŒ‰ ## æ¨™é¡Œåˆ‡å¡Š

    Returns:
        [
            {
                "content": "å€å¡Šå…§å®¹",
                "title": "å€å¡Šæ¨™é¡Œ",
                "source_file": "åŸå§‹æª”æ¡ˆå",
                "file_path": "å®Œæ•´è·¯å¾‘",
                "frontmatter": {...}  # Obsidian metadata
            }
        ]
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()
    except Exception as e:
        print(f"   âš ï¸ ç„¡æ³•è®€å– {file_path}: {e}")
        return []

    # 1. æå– frontmatter
    frontmatter, content = parse_frontmatter(raw_content)

    # 2. æŒ‰ ## æ¨™é¡Œåˆ‡å¡Š
    chunks = []

    # å–å¾—æ–‡ä»¶æ¨™é¡Œï¼ˆ# é–‹é ­çš„ç¬¬ä¸€è¡Œï¼‰
    doc_title = file_path.stem  # é è¨­ç”¨æª”å
    title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
    if title_match:
        doc_title = title_match.group(1).strip()

    # åˆ†å‰²æˆ ## å€å¡Š
    sections = re.split(r'\n## ', content)

    for i, section in enumerate(sections):
        if not section.strip():
            continue

        # æå–æ¨™é¡Œå’Œå…§å®¹
        lines = section.split('\n', 1)
        if i == 0:
            # ç¬¬ä¸€å¡Šå¯èƒ½æ˜¯æ–‡ä»¶é–‹é ­ï¼ˆ# æ¨™é¡Œä¹‹å‰æˆ–ä¹‹å¾Œçš„å…§å®¹ï¼‰
            title = doc_title
            body = section
        else:
            title = lines[0].strip('#').strip()
            body = lines[1] if len(lines) > 1 else ""

        # æ¸…ç† Obsidian èªæ³•
        body = clean_obsidian_syntax(body)

        # éæ¿¾å¤ªçŸ­çš„å€å¡Š
        if len(body.strip()) < MIN_CONTENT_LENGTH:
            continue

        chunks.append({
            "content": body.strip(),
            "title": title,
            "doc_title": doc_title,
            "source_file": file_path.name,
            "file_path": str(file_path),
            "frontmatter": frontmatter
        })

    return chunks


def determine_category(file_path: Path) -> str:
    """æ ¹æ“šæª”æ¡ˆè·¯å¾‘åˆ¤æ–·åˆ†é¡"""
    path_str = str(file_path)

    for keyword, category in CATEGORY_MAP.items():
        if keyword in path_str:
            return category

    # é è¨­åˆ†é¡
    return "service_info"


def determine_service_type(file_path: Path, content: str) -> Optional[str]:
    """æ ¹æ“šæª”æ¡ˆè·¯å¾‘å’Œå…§å®¹åˆ¤æ–·æœå‹™é¡å‹"""
    path_str = str(file_path).lower()
    content_lower = content.lower()

    # é—œéµå­—åˆ¤æ–·
    if any(kw in path_str or kw in content_lower for kw in ["å€Ÿå€", "ç™»è¨˜åœ°å€", "address"]):
        return "address_service"
    if any(kw in path_str or kw in content_lower for kw in ["å…±äº«", "coworking", "å…±åŒå·¥ä½œ"]):
        return "coworking"
    if any(kw in path_str or kw in content_lower for kw in ["è¾¦å…¬å®¤", "office", "ç¨ç«‹è¾¦å…¬"]):
        return "private_office"
    if any(kw in path_str or kw in content_lower for kw in ["æœƒè­°å®¤", "meeting"]):
        return "meeting_room"

    return None


# =============================================================================
# åŒ¯å…¥å™¨
# =============================================================================

class ObsidianImporter:
    """Obsidian çŸ¥è­˜åº«åŒ¯å…¥å™¨"""

    def __init__(self, session: Optional[AsyncSession] = None, dry_run: bool = False):
        self.session = session
        self.dry_run = dry_run
        self.embedding_client = None if dry_run else get_embedding_client()
        self.imported_count = 0
        self.skipped_count = 0
        self.error_count = 0
        self.errors: List[str] = []

    async def import_from_vault(self, vault_path: str) -> Dict[str, Any]:
        """
        å¾ Obsidian vault æ‰¹é‡åŒ¯å…¥çŸ¥è­˜

        Args:
            vault_path: Obsidian vault è·¯å¾‘

        Returns:
            {"imported": N, "skipped": N, "errors": [...]}
        """
        vault = Path(vault_path)
        if not vault.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° Vault: {vault_path}")

        print(f"\nğŸ“‚ æƒæ Vault: {vault_path}")

        # éè¿´æ‰¾æ‰€æœ‰ .md æª”æ¡ˆ
        md_files = list(vault.rglob("*.md"))
        print(f"   æ‰¾åˆ° {len(md_files)} å€‹ Markdown æª”æ¡ˆ")

        for md_file in md_files:
            # è·³éç‰¹å®šæª”æ¡ˆ
            if md_file.name in SKIP_FILES:
                print(f"â­ï¸  è·³éï¼ˆå·²æœ‰æ›´å®Œæ•´ç‰ˆæœ¬ï¼‰: {md_file.name}")
                self.skipped_count += 1
                continue

            # è·³éç‰¹å®šç›®éŒ„
            if any(skip_dir in md_file.parts for skip_dir in SKIP_DIRECTORIES):
                print(f"â­ï¸  è·³éï¼ˆå…§éƒ¨ç‡Ÿé‹è³‡æ–™ï¼‰: {md_file.relative_to(vault)}")
                self.skipped_count += 1
                continue

            # è·³ééš±è—æª”æ¡ˆå’Œ .obsidian ç›®éŒ„
            if any(part.startswith('.') for part in md_file.parts):
                continue

            # è§£æä¸¦åˆ‡å¡Š
            chunks = parse_markdown_to_chunks(md_file)
            if not chunks:
                continue

            print(f"\nğŸ“„ è™•ç†: {md_file.relative_to(vault)}")

            for chunk in chunks:
                await self._add_chunk(
                    content=chunk["content"],
                    category=determine_category(md_file),
                    sub_category=chunk["title"],
                    service_type=determine_service_type(md_file, chunk["content"]),
                    metadata={
                        "source_file": chunk["source_file"],
                        "doc_title": chunk["doc_title"],
                        "file_path": chunk["file_path"],
                        "frontmatter": chunk["frontmatter"],
                        "import_source": "obsidian"
                    }
                )

        # æœ€çµ‚æäº¤
        if not self.dry_run:
            await self.session.commit()

        return {
            "imported": self.imported_count,
            "skipped": self.skipped_count,
            "errors": self.errors
        }

    async def _add_chunk(
        self,
        content: str,
        category: str,
        sub_category: str = None,
        service_type: str = None,
        metadata: Dict = None
    ):
        """æ·»åŠ å–®å€‹çŸ¥è­˜ chunk"""
        if not content or len(content.strip()) < MIN_CONTENT_LENGTH:
            self.skipped_count += 1
            return

        if self.dry_run:
            # Dry Run æ¨¡å¼ä¸æª¢æŸ¥è³‡æ–™åº«é‡è¤‡
            print(f"   ğŸ“ [DRY RUN] å°‡åŒ¯å…¥: {sub_category[:40] if sub_category else content[:40]}...")
            self.imported_count += 1
            return

        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆcontent + category ç›¸åŒè¦–ç‚ºé‡è¤‡ï¼‰
        existing = await self.session.execute(
            select(KnowledgeChunk).where(
                KnowledgeChunk.content == content,
                KnowledgeChunk.category == category
            )
        )
        if existing.scalar_one_or_none():
            print(f"   â­ï¸  é‡è¤‡: {sub_category[:30] if sub_category else 'N/A'}...")
            self.skipped_count += 1
            return

        try:
            # ç”Ÿæˆ Embedding
            embedding = await self.embedding_client.embed_text(content)

            chunk = KnowledgeChunk(
                content=content,
                category=category,
                sub_category=sub_category,
                service_type=service_type,
                extra_data=metadata or {},
                embedding_json=embedding,
                is_active=True
            )

            self.session.add(chunk)
            self.imported_count += 1

            # æ¯ 50 å€‹æäº¤ä¸€æ¬¡
            if self.imported_count % 50 == 0:
                await self.session.commit()
                print(f"   ğŸ’¾ å·²åŒ¯å…¥ {self.imported_count} æ¢...")

            print(f"   âœ… åŒ¯å…¥: {sub_category[:40] if sub_category else content[:40]}...")

        except Exception as e:
            self.error_count += 1
            error_msg = f"{sub_category or content[:30]}: {e}"
            self.errors.append(error_msg)
            print(f"   âŒ éŒ¯èª¤: {error_msg}")


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================

async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="å¾ Obsidian Vault åŒ¯å…¥çŸ¥è­˜åˆ° Brain RAG ç³»çµ±"
    )
    parser.add_argument(
        "vault_path",
        help="Obsidian Vault è·¯å¾‘"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªé è¦½ï¼Œä¸å¯¦éš›å¯«å…¥è³‡æ–™åº«"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("Brain - Obsidian çŸ¥è­˜åº«åŒ¯å…¥å·¥å…·")
    if args.dry_run:
        print("ğŸ” æ¨¡å¼: DRY RUNï¼ˆé è¦½ï¼Œä¸å¯«å…¥ï¼‰")
    else:
        print("ğŸ’¾ æ¨¡å¼: å¯¦éš›åŒ¯å…¥")
    print("=" * 60)

    if args.dry_run:
        # Dry Run æ¨¡å¼ï¼šä¸éœ€è¦è³‡æ–™åº«é€£æ¥
        importer = ObsidianImporter(session=None, dry_run=True)
        try:
            result = await importer.import_from_vault(args.vault_path)
        except FileNotFoundError as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")
            return
    else:
        # å¯¦éš›åŒ¯å…¥æ¨¡å¼ï¼šéœ€è¦è³‡æ–™åº«é€£æ¥
        database_url = settings.DATABASE_URL
        # è½‰æ›ç‚º async driver
        if database_url.startswith("postgresql://"):
            database_url = database_url.replace("postgresql://", "postgresql+asyncpg://", 1)
        elif database_url.startswith("sqlite://") and "+aiosqlite" not in database_url:
            database_url = database_url.replace("sqlite://", "sqlite+aiosqlite://", 1)

        engine = create_async_engine(database_url)
        async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

        # ç¢ºä¿è¡¨å­˜åœ¨
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)

        # åŸ·è¡ŒåŒ¯å…¥
        async with async_session() as session:
            importer = ObsidianImporter(session, dry_run=False)

            try:
                result = await importer.import_from_vault(args.vault_path)
            except FileNotFoundError as e:
                print(f"\nâŒ éŒ¯èª¤: {e}")
                return

        await engine.dispose()

    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 60)
    if args.dry_run:
        print("ğŸ” DRY RUN å®Œæˆï¼ˆæœªå¯¦éš›å¯«å…¥ï¼‰")
    else:
        print("âœ… åŒ¯å…¥å®Œæˆ!")
    print(f"   - {'å°‡åŒ¯å…¥' if args.dry_run else 'æˆåŠŸåŒ¯å…¥'}: {result['imported']} æ¢")
    print(f"   - ç•¥éï¼ˆé‡è¤‡/å¤ªçŸ­ï¼‰: {result['skipped']} æ¢")
    if result['errors']:
        print(f"   - éŒ¯èª¤: {len(result['errors'])} æ¢")
        for err in result['errors'][:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
            print(f"      â€¢ {err}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
